{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "df = pd.read_csv('multiclass_dataset.csv', index_col= 0)\n",
    "df = df.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Hmm...what to make for dinner tonight???  No clue.  Don`t feel like cooking anything    Hubby wants to go away tonight.\"\n",
    "\"Excellent and terbaik...üëçüëçüëç\"\n",
    "\"Long day at office again. Tiring week\"\n",
    "\" I`ll give u an icecream ?\"\n",
    "\"I burnt my arm.  (just thought you should know)\"\n",
    "\"2 days after #all4web ...i`m still tired\"\n",
    "\"@SoChi2 I current use the Nikon D90 and love it, but not as much as the Canon 40D/50D. I chose the D90 for the  video feature. My mistake.\"\n",
    "\"I think i may have found my next apartment. Just one downfall.\"\n",
    "\"Sitting in the car. My ipod is about to die so Im not gonna have anything to do.\"\n",
    " \"Aw poor you..ironing! Eek! lol It`s raining here too.....prob try and get out with the kids later..nt too exciting im afraid!\"\n",
    "\"Thinking of what I should do in Vegas??? Any good ideas or places that are a most see!??\"\n",
    " \"dam living in england  x\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hmm', 'hmm', 'NNS'],\n",
       " ['make', 'make', 'VBP'],\n",
       " ['dinner', 'dinner', 'NN'],\n",
       " ['tonight', 'tonight', 'NN'],\n",
       " ['clue', 'clue', 'NN'],\n",
       " ['feel', 'feel', 'VBP'],\n",
       " ['like', 'like', 'IN'],\n",
       " ['cooking', 'cook', 'VBG'],\n",
       " ['anything', 'anything', 'NN'],\n",
       " ['hubby', 'hubby', 'NN'],\n",
       " ['wants', 'want', 'VBZ'],\n",
       " ['go', 'go', 'VB'],\n",
       " ['away', 'away', 'RB'],\n",
       " ['tonightexcellent', 'tonightexcellent', 'JJ'],\n",
       " ['terbaik', 'terbaik', 'NNS'],\n",
       " ['long', 'long', 'RB'],\n",
       " ['day', 'day', 'NN'],\n",
       " ['office', 'office', 'NN'],\n",
       " ['tiring', 'tire', 'VBG'],\n",
       " ['week', 'week', 'NN'],\n",
       " ['give', 'give', 'VBP'],\n",
       " ['u', 'u', 'JJ'],\n",
       " ['icecream', 'icecream', 'NN'],\n",
       " ['burnt', 'burnt', 'NN'],\n",
       " ['arm', 'arm', 'NN'],\n",
       " ['thought', 'think', 'VBD'],\n",
       " ['know', 'know', 'RB'],\n",
       " ['2', '2', 'CD'],\n",
       " ['days', 'day', 'NNS'],\n",
       " ['all4web', 'all4web', 'RB'],\n",
       " ['still', 'still', 'RB'],\n",
       " ['tired', 'tire', 'VBN'],\n",
       " ['sochi2', 'sochi2', 'JJ'],\n",
       " ['current', 'current', 'JJ'],\n",
       " ['use', 'use', 'NN'],\n",
       " ['nikon', 'nikon', 'NN'],\n",
       " ['d90', 'd90', 'NN'],\n",
       " ['love', 'love', 'VB'],\n",
       " ['much', 'much', 'JJ'],\n",
       " ['canon', 'canon', 'NN'],\n",
       " ['40d50d', '40d50d', 'CD'],\n",
       " ['chose', 'chose', 'JJ'],\n",
       " ['d90', 'd90', 'NNS'],\n",
       " ['video', 'video', 'JJ'],\n",
       " ['feature', 'feature', 'NN'],\n",
       " ['mistakei', 'mistakei', 'NN'],\n",
       " ['think', 'think', 'VBP'],\n",
       " ['may', 'may', 'MD'],\n",
       " ['found', 'find', 'VB'],\n",
       " ['next', 'next', 'JJ'],\n",
       " ['apartment', 'apartment', 'NN'],\n",
       " ['one', 'one', 'CD'],\n",
       " ['downfallsitting', 'downfallsitting', 'NN'],\n",
       " ['car', 'car', 'NN'],\n",
       " ['ipod', 'ipod', 'NN'],\n",
       " ['die', 'die', 'NN'],\n",
       " ['im', 'im', 'NN'],\n",
       " ['gon', 'gon', 'NN'],\n",
       " ['na', 'na', 'TO'],\n",
       " ['anything', 'anything', 'NN'],\n",
       " ['doaw', 'doaw', 'JJ'],\n",
       " ['poor', 'poor', 'JJ'],\n",
       " ['ironing', 'iron', 'VBG'],\n",
       " ['eek', 'eek', 'JJ'],\n",
       " ['lol', 'lol', 'NN'],\n",
       " ['raining', 'rain', 'VBG'],\n",
       " ['prob', 'prob', 'NNS'],\n",
       " ['try', 'try', 'VBP'],\n",
       " ['get', 'get', 'VB'],\n",
       " ['kids', 'kid', 'NNS'],\n",
       " ['later', 'later', 'RB'],\n",
       " ['nt', 'nt', 'VBP'],\n",
       " ['exciting', 'excite', 'VBG'],\n",
       " ['im', 'im', 'NN'],\n",
       " ['afraid', 'afraid', 'JJ'],\n",
       " ['thinking', 'thinking', 'NN'],\n",
       " ['vegas', 'vega', 'NN'],\n",
       " ['good', 'good', 'JJ'],\n",
       " ['ideas', 'idea', 'NNS'],\n",
       " ['places', 'place', 'NNS'],\n",
       " ['see', 'see', 'VBP'],\n",
       " ['dam', 'dam', 'JJ'],\n",
       " ['living', 'living', 'NN'],\n",
       " ['england', 'england', 'NN'],\n",
       " ['x', 'x', 'NN']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hmm...what to make for dinner tonight???  No clue.  Don`t feel like cooking anything    Hubby wants to go away tonight.'\n",
    "text1 = 'Excellent and terbaik...üëçüëçüëç'\n",
    "def cleaning_text(text):\n",
    "    text_pattern = re.compile(\n",
    "        r'(<.+?>)'         # Balises HTML\n",
    "        r'|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'  # Emails\n",
    "        r'|(https?\\W+[^\\s]+)'  # URLs commen√ßant par http ou https\n",
    "        r'|(https?://[^\\s\\n\\r]+)' # URLs commen√ßant par http ou https\n",
    "        r'|(www\\.[^\\s]+)'      # URLs commen√ßant par www\n",
    "        r'|([\\U00010000-\\U0010ffff])'  # √âmojis et autres caract√®res au-del√† de l'ASCII √©tendu\n",
    "        r'|([^\\x00-\\xFF])'     # Tout ce qui n'est pas en ASCII √©tendu (0-255)\n",
    "    )\n",
    "    text = text_pattern.sub('', str(text))\n",
    "    text = text.lower()\n",
    "    punctuation = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      for word in words:\n",
    "        if word not in stop_words:\n",
    "          word = ''.join([c for c in word if c not in punctuation])\n",
    "          if word == '':\n",
    "              continue\n",
    "          tokens.append(word)\n",
    "    \n",
    "    # get the part of speech\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = []\n",
    "    for token, pos in pos_tags:\n",
    "        if pos.startswith('J'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'a')\n",
    "        elif pos.startswith('V'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'v')\n",
    "        elif pos.startswith('RB'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'r')\n",
    "        elif pos.startswith('N'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'n')\n",
    "        else:\n",
    "          lemma = lemmatizer.lemmatize(token)\n",
    "        data.append([token, lemma, pos])\n",
    "    # data.append(['', '', ''])\n",
    "    return data\n",
    "\n",
    "infos = []\n",
    "for text in texts:\n",
    "    data = cleaning_text(text=text)\n",
    "    infos.extend(data)\n",
    "infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_traiter = []\n",
    "# for text, doc in zip(df['text'].to_list(), lemmas):\n",
    "#     if doc == '':\n",
    "#         no_traiter.append(text)\n",
    "# print('le text qui est retourne en chaine vide')\n",
    "# no_traiter\n",
    "# # infos = get_infos(no_traiter)\n",
    "# # infos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
