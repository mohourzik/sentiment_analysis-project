{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all the csv files into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_data = 'data/train.csv'\n",
    "test_data = 'data/test.csv'\n",
    "valid_data = 'data/test.csv'\n",
    "\n",
    "file = \"multiclass_dataset.csv\"\n",
    "\n",
    "def save_load_df(file:str):\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    else:\n",
    "        df = pd.concat(map(pd.read_csv, [train_data, test_data, valid_data]), axis= 0, ignore_index=True)\n",
    "        df.to_csv(file, columns= ['id', 'text', 'label', 'sentiment'])\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    return df\n",
    "\n",
    "df = save_load_df(file=file)\n",
    "# df = df.sample(frac=1, random_state= 1337).reset_index(drop=True)\n",
    "df = df.drop(columns='id')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1e etape: pre-precessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    text_pattern = re.compile(\n",
    "        r'(<.+?>)'         # Balises HTML\n",
    "        r'|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'  # Emails\n",
    "        r'|(https?\\W+[^\\s]+)'  # URLs commençant par http ou https\n",
    "        r'|(https?://[^\\s\\n\\r]+)' # URLs commençant par http ou https\n",
    "        r'|(www\\.[^\\s]+)'      # URLs commençant par www\n",
    "        r'|([\\U00010000-\\U0010ffff])'  # Émojis et autres caractères au-delà de l'ASCII étendu\n",
    "        r'|([^\\x00-\\xFF])'     # Tout ce qui n'est pas en ASCII étendu (0-255)\n",
    "    )\n",
    "    text = text_pattern.sub('', str(text))\n",
    "    text = text.lower()\n",
    "    punctuation = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      for word in words:\n",
    "        if word not in stop_words:\n",
    "          word = ''.join([c for c in word if c not in punctuation])\n",
    "          if word == '':\n",
    "              continue\n",
    "          tokens.append(word)\n",
    "    \n",
    "    # get the part of speech\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = []\n",
    "    for token, pos in pos_tags:\n",
    "        if pos.startswith('J'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'a')\n",
    "        elif pos.startswith('V'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'v')\n",
    "        elif pos.startswith('RB'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'r')\n",
    "        elif pos.startswith('N'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'n')\n",
    "        else:\n",
    "          lemma = lemmatizer.lemmatize(token)\n",
    "        data.append([token, lemma, pos])\n",
    "    data.append(['', '', ''])\n",
    "    return data\n",
    "\n",
    "def get_infos(texts):\n",
    "    infos = []\n",
    "    for text in texts:\n",
    "        data = cleaning_text(text=text)\n",
    "        infos.extend(data)\n",
    "    return infos\n",
    "\n",
    "texts = df['text']\n",
    "df_tokens = pd.DataFrame(get_infos(texts), columns = ['token', 'lemma', 'pos'])\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e etape: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_document(element: list[str]):\n",
    "  docs = []\n",
    "  for i in range(0, len(element)):\n",
    "    for j in range(i, len(element)):\n",
    "      if element[j] == '':\n",
    "        docs.append(' '.join(element[i:j]))\n",
    "        i = j + 1\n",
    "    break\n",
    "  return docs\n",
    "\n",
    "documents = get_document(df_tokens['lemma'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "docs = np.array(documents)\n",
    "tfidf = TfidfVectorizer(use_idf = True, norm = 'l2', smooth_idf=True)\n",
    "tfidf_matrix = tfidf.fit_transform(docs).toarray()\n",
    "csr = csr_matrix(tfidf_matrix ,dtype = float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# save the tfidf_matrix\n",
    "if os.path.exists('tfidf_matrix.npz'):\n",
    "  csr = np.load('tfidf_matrix.npz', allow_pickle=True)\n",
    "else:\n",
    "  np.savez('tfidf_matrix', csr)\n",
    "  csr = np.load('tfidf_matrix.npz', allow_pickle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file_path = r'glove.840B.300d.txt'\n",
    "glove = {}\n",
    "\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove[word] = vector\n",
    "        except ValueError:\n",
    "            print(f\"Skipping malformed line: {line}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vocab_glove = set(glove.keys())\n",
    "glove_embeddings = df_tokens['lemma'].apply(lambda word: glove[word] if word in vocab_glove else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_glove = pd.DataFrame(data=[(lemma, embedding) \n",
    "                              for lemma, embedding in zip(df_tokens['lemma'], glove_embeddings) \n",
    "                              if embedding is not None], \n",
    "                        columns=['lemma', 'glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_vector_glove(phrase, glove, vector_dim=300):\n",
    "    vectors = [glove[word] for word in phrase if word in glove]  \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_dim)  \n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "phr2vec_glove = []\n",
    "for phrase in lemmas:  \n",
    "    if len(phrase) == 0:\n",
    "        continue\n",
    "    mean_vect = get_mean_vector_glove(phrase, glove, vector_dim=300)  \n",
    "    phr2vec_glove.append({\n",
    "        'phrase': phrase,\n",
    "        'phrase2vec': mean_vect,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_glove = pd.DataFrame(phr2vec_glove)\n",
    "df_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embeddings = torch.cat(embeddings, dim = 0)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get a dataframe with lemmes and pos and label :\n",
    "\n",
    "pos = get_document(df_tokens['pos'].tolist())\n",
    "pos = [doc.split() for doc in pos]\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame(data = [(doc, p, label) for doc, p, label in zip(documents, pos, labels)], columns= ['text', 'pos', 'label'])\n",
    "corpus = corpus[corpus['text'] != '']\n",
    "corpus = corpus.reset_index(drop= True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "all_pos_tags = list(set(pos for tag in corpus['pos'] for pos in tag))\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, categories=[all_pos_tags])\n",
    "\n",
    "pos_vectors = []\n",
    "for tags in corpus['pos']:\n",
    "    pos_vectors.append(np.sum(one_hot_encoder.fit_transform([[tag] for tag in tags]), axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase d'entrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df3['Y'] = corpus['label']\n",
    "final_dataset = df3.drop(columns='phrase')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "X = final_dataset['phrase2vec']\n",
    "Y = final_dataset['Y']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, train_size=0.6, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(np.stack(x_train, axis= 0)), torch.tensor(np.stack(y_train, axis=0)))\n",
    "test_dataset = TensorDataset(torch.tensor(np.stack(x_test, axis= 0)), torch.tensor(np.stack(y_test, axis=0)))\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "valid_dataset, test_dataset = random_split(test_dataset, lengths=[0.5, 0.5])\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size= batch_size, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "test_dl  = DataLoader(test_dataset, batch_size= batch_size, shuffle=True, drop_last= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embed_dim = 300, hidden_size = 64, n_layers= 2, dropout = 0.5):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers = n_layers, \n",
    "                            batch_first=True, dropout = dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(n_layers * hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x, (h1, c1) = self.lstm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        logits = self.sigmoid(x)\n",
    "        if y is not None:\n",
    "            # calculate the loss\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # for the distribution of the gradient\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Model()\n",
    "model.to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "print(sum(p.numel() for p in model.parameters()), \"M\")\n",
    "\n",
    "for _ in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for xb_train, yb_train in train_dl:\n",
    "        optimizer.zero_grad(set_to_none=False)\n",
    "        # forward pass\n",
    "        xb_train = xb_train.to(device)\n",
    "        yb_train = yb_train.to(device)\n",
    "        \n",
    "        logits, loss = model(xb_train, yb_train)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        train_acc += (torch.argmax(logits, dim = 1) == yb_train).float().sum().item()\n",
    "        # update the gradient\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "    train_acc /= len(train_dl.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        for xb_valid, yb_valid in valid_dl:\n",
    "            xb_train = xb_train.to(device)\n",
    "            yb_train = yb_train.to(device)\n",
    "            logits, loss = model(xb_valid, yb_valid)\n",
    "            valid_loss += loss.item() * batch_size\n",
    "            valid_acc += (torch.argmax(logits, dim = 1) == yb_train).float().sum().item()\n",
    "    \n",
    "    valid_loss /= len(valid_dl.dataset)\n",
    "    valid_acc /= len(valid_dl.dataset)\n",
    "    print(f'train_loss {train_loss:4f}, valid loss {valid_loss:4f} ')\n",
    "    print(f'accuracy {train_acc:4f} || , valid acc {valid_acc:4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
