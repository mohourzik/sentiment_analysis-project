{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1e etape: pre-precessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31232, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text']\n",
    "\n",
    "def cleaning_text(text):\n",
    "  text_pattern = re.compile(\n",
    "    r'(<.+?>)'         # Balises HTML\n",
    "    r'|([@|#]\\w+)'     # Mentions et hashtags\n",
    "    r'|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'  # Emails\n",
    "    r'|(https?\\W+[^\\s]+)'  # URLs commençant par http ou https\n",
    "    r'|(www\\.[^\\s]+)'      # URLs commençant par www\n",
    "    r'|([\\U00010000-\\U0010ffff])'  # Émojis et autres caractères au-delà de l'ASCII étendu\n",
    "    r'|([^\\x00-\\xFF])'     # Tout ce qui n'est pas en ASCII étendu (0-255)\n",
    "    r'|([^\\w\\s])'          # Tout ce qui n'est pas un caractère alphanumérique ou un espace\n",
    ")\n",
    "  text = text_pattern.sub('', text)\n",
    "  text = text.lower()\n",
    "  punctuation = set(string.punctuation)\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tokens = []\n",
    "  sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "  for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for t in words:\n",
    "      if t in punctuation or t in stop_words:\n",
    "        continue\n",
    "      tokens.append(t)\n",
    "  # get the part of speech\n",
    "  pos_tags = nltk.pos_tag(tokens)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  data = []\n",
    "  for token, pos in pos_tags:\n",
    "    if pos.startswith('J'):\n",
    "      lemma = lemmatizer.lemmatize(token, pos = 'a')\n",
    "    elif pos.startswith('V'):\n",
    "      lemma = lemmatizer.lemmatize(token, pos = 'v')\n",
    "    elif pos.startswith('R'):\n",
    "      lemma = lemmatizer.lemmatize(token, pos = 'r')\n",
    "    elif pos.startswith('N'):\n",
    "      lemma = lemmatizer.lemmatize(token, pos = 'n')\n",
    "    else:\n",
    "      lemma = lemmatizer.lemmatize(token)\n",
    "    data.append([token, lemma, pos])\n",
    "\n",
    "  return data\n",
    "\n",
    "def get_info():\n",
    "  data = []\n",
    "  for text in texts:\n",
    "    text = cleaning_text(text)\n",
    "    data.extend(text)\n",
    "    data.append(['', '', ''])\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./tokens.csv\"):\n",
    "  df_tokens = pd.read_csv(\"./tokens.csv\")\n",
    "else:\n",
    "  data = get_info()\n",
    "  df_tokens = pd.DataFrame(data, columns = ['token', 'lemma', 'pos'])\n",
    "  df_tokens.to_csv(\"./tokens.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = pd.read_csv('./tokens.csv')\n",
    "df_tokens.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cooking</td>\n",
       "      <td>cook</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microwave</td>\n",
       "      <td>microwave</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pizzas</td>\n",
       "      <td>pizza</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yummy</td>\n",
       "      <td>yummy</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338804</th>\n",
       "      <td>hours</td>\n",
       "      <td>hour</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338805</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338806</th>\n",
       "      <td>missed</td>\n",
       "      <td>miss</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338807</th>\n",
       "      <td>play</td>\n",
       "      <td>play</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338808</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>338809 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token      lemma  pos\n",
       "0         cooking       cook  VBG\n",
       "1       microwave  microwave   NN\n",
       "2          pizzas      pizza   NN\n",
       "3           yummy      yummy   NN\n",
       "4                                \n",
       "...           ...        ...  ...\n",
       "338804      hours       hour  NNS\n",
       "338805                           \n",
       "338806     missed       miss  VBN\n",
       "338807       play       play   NN\n",
       "338808                           \n",
       "\n",
       "[338809 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e etape: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df_tokens['token'].tolist()\n",
    "\n",
    "def get_document(tokens: list[str]):\n",
    "  docs = []\n",
    "  for i in range(0, len(tokens)):\n",
    "    for j in range(i, len(tokens)):\n",
    "      if tokens[j] == '':\n",
    "        docs.append(' '.join(tokens[i:j]))\n",
    "        i = j + 1\n",
    "    break\n",
    "  return docs\n",
    "\n",
    "docs = get_document(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "docs = np.array(docs)\n",
    "tfidf = TfidfVectorizer(use_idf = True, norm = 'l2', smooth_idf=True)\n",
    "tfidf_matrix = tfidf.fit_transform(docs).toarray()\n",
    "csr = csr_matrix(tfidf_matrix ,dtype = float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tfidf_matrix\n",
    "\n",
    "if os.path.exists('./tfidf_matrix.npz'):\n",
    "  csr = np.load('./tfidf_matrix.npz', allow_pickle=True)\n",
    "else:\n",
    "  np.savez('./tfidf_matrix', csr)\n",
    "  csr = np.load('./tfidf_matrix.npz', allow_pickle= True)\n",
    "csr = csr['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3e etape: WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_document(tokens= tokens)\n",
    "tokens = [doc.split() for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "path = './GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(path, binary= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510956883430481),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204220056533813),\n",
       " ('prince', 0.6159993410110474)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('king',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76640123"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
