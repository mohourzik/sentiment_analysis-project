{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger_eng: <urlopen\n",
      "[nltk_data]     error [Errno 11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cooking microwave pizzas, yummy</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Any plans of allowing sub tasks to show up in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the humor, I just reworded it. Like sa...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naw idk what ur talkin about</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That sucks to hear. I hate days like that</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41639</th>\n",
       "      <td>Fuck no internet damn time warner!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41640</th>\n",
       "      <td>Looking forward to android 1.5 being pushed t...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41641</th>\n",
       "      <td>Not good. Wasted time.</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41642</th>\n",
       "      <td>U were great, as always. But, can`t we do an ...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41643</th>\n",
       "      <td>- Love your desserts. Used to live in OR but ...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41644 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label sentiment\n",
       "0                        Cooking microwave pizzas, yummy      2  positive\n",
       "1      Any plans of allowing sub tasks to show up in ...      1   neutral\n",
       "2       I love the humor, I just reworded it. Like sa...      2  positive\n",
       "3                           naw idk what ur talkin about      1   neutral\n",
       "4              That sucks to hear. I hate days like that      0  negative\n",
       "...                                                  ...    ...       ...\n",
       "41639                 Fuck no internet damn time warner!      0  negative\n",
       "41640   Looking forward to android 1.5 being pushed t...      1   neutral\n",
       "41641                             Not good. Wasted time.      0  negative\n",
       "41642   U were great, as always. But, can`t we do an ...      2  positive\n",
       "41643   - Love your desserts. Used to live in OR but ...      2  positive\n",
       "\n",
       "[41644 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = 'data/train.csv'\n",
    "test_data = 'data/test.csv'\n",
    "valid_data = 'data/test.csv'\n",
    "\n",
    "file = \"multiclass_dataset.csv\"\n",
    "\n",
    "def save_load_df(file:str):\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    else:\n",
    "        df = pd.concat(map(pd.read_csv, [train_data, test_data, valid_data]), axis= 0, ignore_index=True)\n",
    "        df.to_csv(file, columns= ['id', 'text', 'label', 'sentiment'])\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    return df\n",
    "\n",
    "df = save_load_df(file=file)\n",
    "# df = df.sample(frac=1, random_state= 1337).reset_index(drop=True)\n",
    "df = df.drop(columns='id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cooking</td>\n",
       "      <td>cook</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microwave</td>\n",
       "      <td>microwave</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pizzas</td>\n",
       "      <td>pizza</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yummy</td>\n",
       "      <td>yummy</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447665</th>\n",
       "      <td>live</td>\n",
       "      <td>live</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447666</th>\n",
       "      <td>live</td>\n",
       "      <td>live</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447667</th>\n",
       "      <td>tx</td>\n",
       "      <td>tx</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447668</th>\n",
       "      <td>visit</td>\n",
       "      <td>visit</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447669</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447670 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token      lemma  pos\n",
       "0         cooking       cook  VBG\n",
       "1       microwave  microwave   NN\n",
       "2          pizzas      pizza   NN\n",
       "3           yummy      yummy   NN\n",
       "4                                \n",
       "...           ...        ...  ...\n",
       "447665       live       live   JJ\n",
       "447666       live       live   JJ\n",
       "447667         tx         tx   NN\n",
       "447668      visit      visit   NN\n",
       "447669                           \n",
       "\n",
       "[447670 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_text(text):\n",
    "    text_pattern = re.compile(\n",
    "        r'(<.+?>)'         # Balises HTML\n",
    "        r'|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'  # Emails\n",
    "        r'|(https?\\W+[^\\s]+)'  # URLs commençant par http ou https\n",
    "        r'|(https?://[^\\s\\n\\r]+)' # URLs commençant par http ou https\n",
    "        r'|(www\\.[^\\s]+)'      # URLs commençant par www\n",
    "        r'|([\\U00010000-\\U0010ffff])'  # Émojis et autres caractères au-delà de l'ASCII étendu\n",
    "        r'|([^\\x00-\\xFF])'     # Tout ce qui n'est pas en ASCII étendu (0-255)\n",
    "    )\n",
    "    text = text_pattern.sub('', str(text))\n",
    "    text = text.lower()\n",
    "    punctuation = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      for word in words:\n",
    "        if word not in stop_words:\n",
    "          word = ''.join([c for c in word if c not in punctuation])\n",
    "          if word == '':\n",
    "              continue\n",
    "          tokens.append(word)\n",
    "    \n",
    "    # get the part of speech\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = []\n",
    "    for token, pos in pos_tags:\n",
    "        if pos.startswith('J'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'a')\n",
    "        elif pos.startswith('V'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'v')\n",
    "        elif pos.startswith('RB'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'r')\n",
    "        elif pos.startswith('N'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'n')\n",
    "        else:\n",
    "          lemma = lemmatizer.lemmatize(token)\n",
    "        data.append([token, lemma, pos])\n",
    "    data.append(['', '', ''])\n",
    "    return data\n",
    "\n",
    "def get_infos(texts):\n",
    "    infos = []\n",
    "    for text in texts:\n",
    "        data = cleaning_text(text=text)\n",
    "        infos.extend(data)\n",
    "    return infos\n",
    "\n",
    "texts = df['text']\n",
    "df_tokens = pd.DataFrame(get_infos(texts), columns = ['token', 'lemma', 'pos'])\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_document(element: list[str]):\n",
    "  docs = []\n",
    "  for i in range(0, len(element)):\n",
    "    for j in range(i, len(element)):\n",
    "      if element[j] == '':\n",
    "        docs.append(' '.join(element[i:j]))\n",
    "        i = j + 1\n",
    "    break\n",
    "  return docs\n",
    "\n",
    "documents = get_document(df_tokens['lemma'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "docs = np.array(documents)\n",
    "tfidf = TfidfVectorizer(use_idf = True, norm = 'l2', smooth_idf=True)\n",
    "tfidf_matrix = tfidf.fit_transform(docs).toarray()\n",
    "csr = csr_matrix(tfidf_matrix ,dtype = float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tfidf_matrix\n",
    "if os.path.exists('tfidf_matrix.npz'):\n",
    "  csr = np.load('tfidf_matrix.npz', allow_pickle=True)\n",
    "else:\n",
    "  np.savez('tfidf_matrix', csr)\n",
    "  csr = np.load('tfidf_matrix.npz', allow_pickle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FASTTEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "fasttext_model_path = 'D:\\\\sentiment_analysis Project\\\\sentiment_analysis-project\\\\cc.en.300.bin'  # Chemin vers le fichier binaire FastText\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = df_tokens['lemma'].apply(lambda word: fasttext_model[word] if word in vocab else None)\n",
    "df7 = pd.DataFrame(data = [(lemma, fasttext) \\\n",
    "                           for lemma, fasttext in zip(df_tokens['lemma'], fasttext) \\\n",
    "                            if fasttext is not None], columns= ['lemma', 'fasttext'])\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phr2vec = []\n",
    "for phrase in lemmas:\n",
    "    if len(phrase) == 0:\n",
    "        continue\n",
    "    mean_vect = fasttext_model.get_mean_vector(keys = phrase, pre_normalize = False)\n",
    "    phr2vec.append({\n",
    "        'phrase': phrase,\n",
    "        'phrase2vec': mean_vect,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Contextuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "tokens = get_document(df_tokens['token'].tolist())\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "embeddings = []\n",
    "i = 0\n",
    "for phrase in tokens:\n",
    "    words = tokenizer(phrase, return_tensors='pt')\n",
    "    # feed to the embedding layer to get the embdeddings for each token\n",
    "    token_embd = model(**words)\n",
    "    embedding = token_embd.last_hidden_state\n",
    "    # sum up all the tokens embedding to get the phrase embedding\n",
    "    phrase_embd = embedding.mean(dim = 1)\n",
    "    embeddings.append(phrase_embd)\n",
    "    i +=1 \n",
    "    if i == 64:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embeddings = torch.cat(embeddings, dim = 0)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dataframe with lemmes and pos and label :\n",
    "\n",
    "pos = get_document(df_tokens['pos'].tolist())\n",
    "pos = [doc.split() for doc in pos]\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame(data = [(doc, p, label) for doc, p, label in zip(documents, pos, labels)], columns= ['text', 'pos', 'label'])\n",
    "corpus = corpus[corpus['text'] != '']\n",
    "corpus = corpus.reset_index(drop= True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "all_pos_tags = list(set(pos for tag in corpus['pos'] for pos in tag))\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, categories=[all_pos_tags])\n",
    "\n",
    "pos_vectors = []\n",
    "for tags in corpus['pos']:\n",
    "    pos_vectors.append(np.sum(one_hot_encoder.fit_transform([[tag] for tag in tags]), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_pos = pd.DataFrame(pos_vectors, columns= all_pos_tags)\n",
    "# df_pos['text'] = corpus['text']\n",
    "# df_pos['label'] = corpus['label']\n",
    "\n",
    "# df_pos = df_pos[[\"text\"] + all_pos_tags + [\"label\"]]\n",
    "# df_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Y'] = corpus['label']\n",
    "final_dataset = df3.drop(columns='phrase')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = final_dataset['phrase2vec']\n",
    "Y = final_dataset['Y']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, train_size=0.7, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size = 16):\n",
    "    xb, yb = list(x[i:i+batch_size]), list(y[i:i+batch_size])\n",
    "    xb = torch.tensor(np.stack(xb, axis= 0), dtype= torch.float32)\n",
    "    yb = torch.tensor(np.stack(yb, axis= 0), dtype= torch.long)\n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch(x_train, y_train)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size = 128, n_layers= 2):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers = n_layers, \n",
    "                            batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(n_layers * hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h1, c1) = self.lstm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        logits = self.sigmoid(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(x_train, y_train, batch_size= 32)\n",
    "model = Model(embed_dim=xb.size(1))\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad(set_to_none=False)\n",
    "    # forward pass\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update the gradient\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
