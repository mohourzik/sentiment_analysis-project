{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all the csv files into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cooking microwave pizzas, yummy</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Any plans of allowing sub tasks to show up in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the humor, I just reworded it. Like sa...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naw idk what ur talkin about</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That sucks to hear. I hate days like that</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41639</th>\n",
       "      <td>Fuck no internet damn time warner!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41640</th>\n",
       "      <td>Looking forward to android 1.5 being pushed t...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41641</th>\n",
       "      <td>Not good. Wasted time.</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41642</th>\n",
       "      <td>U were great, as always. But, can`t we do an ...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41643</th>\n",
       "      <td>- Love your desserts. Used to live in OR but ...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41644 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label sentiment\n",
       "0                        Cooking microwave pizzas, yummy      2  positive\n",
       "1      Any plans of allowing sub tasks to show up in ...      1   neutral\n",
       "2       I love the humor, I just reworded it. Like sa...      2  positive\n",
       "3                           naw idk what ur talkin about      1   neutral\n",
       "4              That sucks to hear. I hate days like that      0  negative\n",
       "...                                                  ...    ...       ...\n",
       "41639                 Fuck no internet damn time warner!      0  negative\n",
       "41640   Looking forward to android 1.5 being pushed t...      1   neutral\n",
       "41641                             Not good. Wasted time.      0  negative\n",
       "41642   U were great, as always. But, can`t we do an ...      2  positive\n",
       "41643   - Love your desserts. Used to live in OR but ...      2  positive\n",
       "\n",
       "[41644 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = 'data/train.csv'\n",
    "test_data = 'data/test.csv'\n",
    "valid_data = 'data/test.csv'\n",
    "\n",
    "file = \"multiclass_dataset.csv\"\n",
    "\n",
    "def save_load_df(file:str):\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    else:\n",
    "        df = pd.concat(map(pd.read_csv, [train_data, test_data, valid_data]), axis= 0, ignore_index=True)\n",
    "        df.to_csv(file, columns= ['id', 'text', 'label', 'sentiment'])\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    return df\n",
    "\n",
    "df = save_load_df(file=file)\n",
    "# df = df.sample(frac=1, random_state= 1337).reset_index(drop=True)\n",
    "df = df.drop(columns='id')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1e etape: pre-precessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cooking</td>\n",
       "      <td>cook</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microwave</td>\n",
       "      <td>microwave</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pizzas</td>\n",
       "      <td>pizza</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yummy</td>\n",
       "      <td>yummy</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447665</th>\n",
       "      <td>live</td>\n",
       "      <td>live</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447666</th>\n",
       "      <td>live</td>\n",
       "      <td>live</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447667</th>\n",
       "      <td>tx</td>\n",
       "      <td>tx</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447668</th>\n",
       "      <td>visit</td>\n",
       "      <td>visit</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447669</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447670 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token      lemma  pos\n",
       "0         cooking       cook  VBG\n",
       "1       microwave  microwave   NN\n",
       "2          pizzas      pizza   NN\n",
       "3           yummy      yummy   NN\n",
       "4                                \n",
       "...           ...        ...  ...\n",
       "447665       live       live   JJ\n",
       "447666       live       live   JJ\n",
       "447667         tx         tx   NN\n",
       "447668      visit      visit   NN\n",
       "447669                           \n",
       "\n",
       "[447670 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_text(text):\n",
    "    text_pattern = re.compile(\n",
    "        r'(<.+?>)'         # Balises HTML\n",
    "        r'|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'  # Emails\n",
    "        r'|(https?\\W+[^\\s]+)'  # URLs commençant par http ou https\n",
    "        r'|(https?://[^\\s\\n\\r]+)' # URLs commençant par http ou https\n",
    "        r'|(www\\.[^\\s]+)'      # URLs commençant par www\n",
    "        r'|([\\U00010000-\\U0010ffff])'  # Émojis et autres caractères au-delà de l'ASCII étendu\n",
    "        r'|([^\\x00-\\xFF])'     # Tout ce qui n'est pas en ASCII étendu (0-255)\n",
    "    )\n",
    "    text = text_pattern.sub('', str(text))\n",
    "    text = text.lower()\n",
    "    punctuation = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      for word in words:\n",
    "        if word not in stop_words:\n",
    "          word = ''.join([c for c in word if c not in punctuation])\n",
    "          if word == '':\n",
    "              continue\n",
    "          tokens.append(word)\n",
    "    \n",
    "    # get the part of speech\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = []\n",
    "    for token, pos in pos_tags:\n",
    "        if pos.startswith('J'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'a')\n",
    "        elif pos.startswith('V'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'v')\n",
    "        elif pos.startswith('RB'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'r')\n",
    "        elif pos.startswith('N'):\n",
    "          lemma = lemmatizer.lemmatize(token, pos = 'n')\n",
    "        else:\n",
    "          lemma = lemmatizer.lemmatize(token)\n",
    "        data.append([token, lemma, pos])\n",
    "    data.append(['', '', ''])\n",
    "    return data\n",
    "\n",
    "def get_infos(texts):\n",
    "    infos = []\n",
    "    for text in texts:\n",
    "        data = cleaning_text(text=text)\n",
    "        infos.extend(data)\n",
    "    return infos\n",
    "\n",
    "texts = df['text']\n",
    "df_tokens = pd.DataFrame(get_infos(texts), columns = ['token', 'lemma', 'pos'])\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e etape: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_document(element: list[str]):\n",
    "  docs = []\n",
    "  for i in range(0, len(element)):\n",
    "    for j in range(i, len(element)):\n",
    "      if element[j] == '':\n",
    "        docs.append(' '.join(element[i:j]))\n",
    "        i = j + 1\n",
    "    break\n",
    "  return docs\n",
    "\n",
    "documents = get_document(df_tokens['lemma'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "docs = np.array(documents)\n",
    "tfidf = TfidfVectorizer(use_idf = True, norm = 'l2', smooth_idf=True)\n",
    "tfidf_matrix = tfidf.fit_transform(docs).toarray()\n",
    "csr = csr_matrix(tfidf_matrix ,dtype = float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tfidf_matrix\n",
    "if os.path.exists('tfidf_matrix.npz'):\n",
    "  csr = np.load('tfidf_matrix.npz', allow_pickle=True)\n",
    "else:\n",
    "  np.savez('tfidf_matrix', csr)\n",
    "  csr = np.load('tfidf_matrix.npz', allow_pickle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FASTTEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\sentiment_analysis Project\\\\sentiment_analysis-project\\\\cc.en.300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m      4\u001b[0m fasttext_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msentiment_analysis Project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msentiment_analysis-project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcc.en.300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Chemin vers le fichier binaire FastText\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m fasttext_model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfasttext_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[0;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[1;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[0;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\smart_open\\smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\smart_open\\smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\sentiment_analysis Project\\\\sentiment_analysis-project\\\\cc.en.300.bin'"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "fasttext_model_path = 'D:\\\\sentiment_analysis Project\\\\sentiment_analysis-project\\\\cc.en.300.bin'  # Chemin vers le fichier binaire FastText\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(list(fasttext_model.key_to_index.keys()))\n",
    "lemmas = [doc.split() for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = df_tokens['lemma'].apply(lambda word: fasttext_model[word] if word in vocab else None)\n",
    "df7 = pd.DataFrame(data = [(lemma, fasttext) \\\n",
    "                           for lemma, fasttext in zip(df_tokens['lemma'], fasttext) \\\n",
    "                            if fasttext is not None], columns= ['lemma', 'fasttext'])\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phr2vec = []\n",
    "for phrase in lemmas:\n",
    "    if len(phrase) == 0:\n",
    "        continue\n",
    "    mean_vect = fasttext_model.get_mean_vector(keys = phrase, pre_normalize = False)\n",
    "    phr2vec.append({\n",
    "        'phrase': phrase,\n",
    "        'phrase2vec': mean_vect,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Contextuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "tokens = get_document(df_tokens['token'].tolist())\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "embeddings = []\n",
    "i = 0\n",
    "for phrase in tokens:\n",
    "    words = tokenizer(phrase, return_tensors='pt')\n",
    "    # feed to the embedding layer to get the embdeddings for each token\n",
    "    token_embd = model(**words)\n",
    "    embedding = token_embd.last_hidden_state\n",
    "    # sum up all the tokens embedding to get the phrase embedding\n",
    "    phrase_embd = embedding.mean(dim = 1)\n",
    "    embeddings.append(phrase_embd)\n",
    "    i +=1 \n",
    "    if i == 64:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embeddings = torch.cat(embeddings, dim = 0)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dataframe with lemmes and pos and label :\n",
    "\n",
    "pos = get_document(df_tokens['pos'].tolist())\n",
    "pos = [doc.split() for doc in pos]\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame(data = [(doc, p, label) for doc, p, label in zip(documents, pos, labels)], columns= ['text', 'pos', 'label'])\n",
    "corpus = corpus[corpus['text'] != '']\n",
    "corpus = corpus.reset_index(drop= True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "all_pos_tags = list(set(pos for tag in corpus['pos'] for pos in tag))\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, categories=[all_pos_tags])\n",
    "\n",
    "pos_vectors = []\n",
    "for tags in corpus['pos']:\n",
    "    pos_vectors.append(np.sum(one_hot_encoder.fit_transform([[tag] for tag in tags]), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_pos = pd.DataFrame(pos_vectors, columns= all_pos_tags)\n",
    "# df_pos['text'] = corpus['text']\n",
    "# df_pos['label'] = corpus['label']\n",
    "\n",
    "# df_pos = df_pos[[\"text\"] + all_pos_tags + [\"label\"]]\n",
    "# df_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase d'entrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Y'] = corpus['label']\n",
    "final_dataset = df3.drop(columns='phrase')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = final_dataset['phrase2vec']\n",
    "Y = final_dataset['Y']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, train_size=0.7, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size = 16):\n",
    "    xb, yb = list(x[i:i+batch_size]), list(y[i:i+batch_size])\n",
    "    xb = torch.tensor(np.stack(xb, axis= 0), dtype= torch.float32)\n",
    "    yb = torch.tensor(np.stack(yb, axis= 0), dtype= torch.long)\n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch(x_train, y_train)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_size = 128, n_layers= 2):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers = n_layers, \n",
    "                            batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(n_layers * hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h1, c1) = self.lstm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        logits = self.sigmoid(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(x_train, y_train, batch_size= 32)\n",
    "model = Model(embed_dim=xb.size(1))\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad(set_to_none=False)\n",
    "    # forward pass\n",
    "    logits = model(xb)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update the gradient\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
