{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /teamspace/studios/this_studio/sentiment_analysis-project\n",
    "train_data = './sentiment_analysis-project/data/train.csv'\n",
    "test_data = './sentiment_analysis-project/data/test.csv'\n",
    "valid_data = './sentiment_analysis-project/data/validation.csv'\n",
    "\n",
    "file = './sentiment_analysis-project/multiclass_dataset.csv'\n",
    "\n",
    "def save_load_df(file:str):\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    else:\n",
    "        df = pd.concat(map(pd.read_csv, [train_data, test_data, valid_data]), axis= 0, ignore_index=True)\n",
    "        df.to_csv(file, columns= ['id', 'text', 'label', 'sentiment'])\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    return df\n",
    "\n",
    "df = save_load_df(file=file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP1, TP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess():\n",
    "    def __init__(self):\n",
    "        self.text_pattern = re.compile(\n",
    "        r'(<.+?>)'         # Balises HTML\n",
    "        r'(#|@)\\w+'  # @ and # words\n",
    "        r'|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'  # Emails\n",
    "        r'|(https?://[^\\s\\n\\r]+)' # URLs commençant par http ou https\n",
    "        r'|(www\\.[^\\s]+)'      # URLs commençant par www\n",
    "        r'|([\\U00010000-\\U0010ffff])'  # Émojis et autres caractères au-delà de l'ASCII étendu\n",
    "        r'|([^\\x00-\\xFF])'     # Tout ce qui n'est pas en ASCII étendu (0-255)\n",
    "        )\n",
    "        self.emoji_pattern = re.compile(\n",
    "            \"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols and Pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "            \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "            \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.punctuation = set(string.punctuation)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = self.text_pattern.sub('', str(text))\n",
    "        text = self.emoji_pattern.sub('', str(text))\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    \n",
    "    def get_tokens(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                if word not in self.stop_words:\n",
    "                    # clean all the punctuation and the StopWords\n",
    "                    word = ''.join([c for c in word if c not in self.punctuation])\n",
    "                    if word == '':\n",
    "                        continue\n",
    "                    tokens.append(word)\n",
    "        return tokens\n",
    "\n",
    "    def lemmetize_with_pos(self, tokens):\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        lemmes = [] \n",
    "        pos_tag = []\n",
    "        for token, pos in pos_tags:\n",
    "            if pos.startswith('J'):\n",
    "                lemma = self.lemmatizer.lemmatize(token, pos = 'a')\n",
    "            elif pos.startswith('V'):\n",
    "                lemma =  self.lemmatizer.lemmatize(token, pos = 'v')\n",
    "            elif pos.startswith('RB'):\n",
    "                lemma = self.lemmatizer.lemmatize(token, pos = 'r')\n",
    "            elif pos.startswith('N'):\n",
    "                lemma = self.lemmatizer.lemmatize(token, pos = 'n')\n",
    "            else:\n",
    "                lemma = self.lemmatizer.lemmatize(token)\n",
    "            lemmes.append(lemma)\n",
    "            pos_tag.append(pos)\n",
    "        return lemmes, pos_tag\n",
    "    \n",
    "    def get_lemmes(self, text):\n",
    "        tokens = self.get_tokens(text)\n",
    "        lemmes, _ = self.lemmetize_with_pos(tokens)\n",
    "        return lemmes\n",
    "    \n",
    "    def visualize_data(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        tokens = self.get_tokens(text)\n",
    "        lemmes, pos_tag = self.lemmetize_with_pos(tokens)\n",
    "        data = [[token, lemme, pos] for token, lemme, pos in zip(tokens, lemmes, pos_tag)]\n",
    "        return data\n",
    "    \n",
    "texts = list(df['text'])\n",
    "labels = list(df['label'])\n",
    "process_text = PreProcess()\n",
    "corpus = pd.DataFrame(data = [(process_text.get_tokens(text), label) for text, label in zip(texts, labels)], columns=['sentence', 'label'])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the empty rows\n",
    "corpus = corpus[corpus['sentence'].str.len() != 0]\n",
    "corpus = corpus.reset_index(drop = True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove_file = './sentiment_analysis-project/glove.6B.300d.txt'\n",
    "glove_model = {}\n",
    "\n",
    "with open(glove_file, 'r') as f:\n",
    "  for line in f.readlines():\n",
    "    line = line.split(' ')\n",
    "    try:\n",
    "      glove_model[line[0]] = np.array(line[1:], dtype=float)\n",
    "    except:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(corpus['sentence'])\n",
    "\n",
    "def sentence_to_vec(sentence):\n",
    "    sentence_to_vec = [glove_model[word] for word in sentence if word in glove_model]\n",
    "    return sentence_to_vec\n",
    "\n",
    "df1 = pd.DataFrame(data = [(sentence_to_vec(sentence), label) for sentence, label in zip(sentences, list(corpus['label']))], \n",
    "                   columns=['vec', 'label'])\n",
    "\n",
    "df1 = df1[df1['vec'].str.len() != 0]\n",
    "# df1 = df1.sort_values(by = 'vec', key = lambda vec: vec.str.len(), ascending=False)\n",
    "df1 = df1.reset_index(drop= True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(vec) for vec in list(df1['vec']) if len(vec) <= 64]\n",
    "# df1 = df1[df1['vec'].str.len() <= 64]\n",
    "# df1 = df1.reset_index(drop= True)\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "X = pad_sequence([torch.from_numpy(np.array(vec, dtype = np.float32)) for vec in list(df1['vec'])], batch_first=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.from_numpy(np.array(list(df1['label'])))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=42, shuffle=True)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size=0.5, train_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_dataset, batch_size= batch_size, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, drop_last=True)\n",
    "test_dl  = DataLoader(test_dataset, batch_size= batch_size, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# (258, 195, 300)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim = 300,\n",
    "                 hidden_size = 64,\n",
    "                 bidirectional = False,\n",
    "                 num_layers = 2,\n",
    "                 dropout = 0.2\n",
    "        ):\n",
    "        super(Model, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size= embed_dim,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias= False,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.D = 1 if self.bidirectional == False else 2\n",
    "        self.fc2 = nn.Linear(self.D * hidden_size , 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        out, (h1, c1) = self.lstm(x) \n",
    "        out = F.softmax(self.fc2(out), dim = -1)\n",
    "        return out\n",
    "\n",
    "torch.manual_seed(42) # for the distribution of the gradient\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = Model()\n",
    "model.to(device=device)\n",
    "# model = torch.compile(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 3e-4)\n",
    "print(sum(p.numel() for p in model.parameters()),'parameters')\n",
    "\n",
    "import time\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "num_iter = 50\n",
    "for i in range(num_iter):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    t1 = time.time()\n",
    "    model.train()\n",
    "    for xb_train, yb_train in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        xb_train = xb_train.to(device)\n",
    "        yb_train = yb_train.to(device)\n",
    "        with torch.autocast(device_type = 'cuda', dtype = torch.float16):\n",
    "            logits= model(xb_train)[:, -1, :]\n",
    "            loss = F.cross_entropy(logits, yb_train)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        train_acc += (torch.argmax(logits, dim = 1) == yb_train).float().sum().item()\n",
    "        # update the gradient\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "    train_acc /= len(train_dl.dataset)\n",
    "     \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for xb_valid, yb_valid in valid_dl:\n",
    "            xb_valid = xb_valid.to(device)\n",
    "            yb_valid = yb_valid.to(device)\n",
    "            logits = model(xb_valid)[:, -1, :]\n",
    "            loss = F.cross_entropy(logits, yb_valid)\n",
    "            valid_loss += loss.item() * batch_size\n",
    "            valid_acc += (torch.argmax(logits, dim = 1) == yb_valid).float().sum().item()\n",
    "    \n",
    "    valid_loss /= len(valid_dl.dataset)\n",
    "    valid_acc /= len(valid_dl.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1)\n",
    "    print(f'epoch {i} || train_loss {train_loss:.3f}, valid_loss {valid_loss:.3f}, train_accu {train_acc:.3f} , valid_accu {valid_acc:.3f}, dt= {dt:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_arr = np.arange(num_iter) + 1\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, train_losses, '-o', label='Train loss')\n",
    "ax.plot(x_arr, valid_losses, '--<', label='Validation loss')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlabel('Epoch', size=10)\n",
    "ax.set_ylabel('Loss', size=10)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, train_accuracies, '-o', label='Train acc.')\n",
    "ax.plot(x_arr, valid_accuracies, '--<', label='Validation acc.')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlabel('Epoch', size=10)\n",
    "ax.set_ylabel('Accuracy', size=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "test_acc = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for xb_test, yb_test in test_dl:\n",
    "        xb_test = xb_test.to(device)\n",
    "        yb_test = yb_test.to(device)\n",
    "        logits = model(xb_test)[:, -1, :]\n",
    "        pred = torch.argmax(logits, dim = 1)\n",
    "        test_acc += (pred == yb_test).float().sum().item()\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(yb_test.cpu().numpy())\n",
    "\n",
    "test_acc /= len(test_dl.dataset)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')  # 'weighted' pour pondérer selon les classes\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print('test accuracy', test_acc)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
