{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9536</td>\n",
       "      <td>Cooking microwave pizzas, yummy</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6135</td>\n",
       "      <td>Any plans of allowing sub tasks to show up in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17697</td>\n",
       "      <td>I love the humor, I just reworded it. Like sa...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14182</td>\n",
       "      <td>naw idk what ur talkin about</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17840</td>\n",
       "      <td>That sucks to hear. I hate days like that</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41639</th>\n",
       "      <td>10277</td>\n",
       "      <td>Fuck no internet damn time warner!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41640</th>\n",
       "      <td>8610</td>\n",
       "      <td>Looking forward to android 1.5 being pushed t...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41641</th>\n",
       "      <td>8114</td>\n",
       "      <td>Not good. Wasted time.</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41642</th>\n",
       "      <td>3034</td>\n",
       "      <td>U were great, as always. But, can`t we do an ...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41643</th>\n",
       "      <td>11601</td>\n",
       "      <td>- Love your desserts. Used to live in OR but ...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41644 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  label  \\\n",
       "0       9536                    Cooking microwave pizzas, yummy      2   \n",
       "1       6135  Any plans of allowing sub tasks to show up in ...      1   \n",
       "2      17697   I love the humor, I just reworded it. Like sa...      2   \n",
       "3      14182                       naw idk what ur talkin about      1   \n",
       "4      17840          That sucks to hear. I hate days like that      0   \n",
       "...      ...                                                ...    ...   \n",
       "41639  10277                 Fuck no internet damn time warner!      0   \n",
       "41640   8610   Looking forward to android 1.5 being pushed t...      1   \n",
       "41641   8114                             Not good. Wasted time.      0   \n",
       "41642   3034   U were great, as always. But, can`t we do an ...      2   \n",
       "41643  11601   - Love your desserts. Used to live in OR but ...      2   \n",
       "\n",
       "      sentiment  \n",
       "0      positive  \n",
       "1       neutral  \n",
       "2      positive  \n",
       "3       neutral  \n",
       "4      negative  \n",
       "...         ...  \n",
       "41639  negative  \n",
       "41640   neutral  \n",
       "41641  negative  \n",
       "41642  positive  \n",
       "41643  positive  \n",
       "\n",
       "[41644 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /teamspace/studios/this_studio/sentiment_analysis-project\n",
    "train_data = './sentiment_analysis-project/data/train.csv'\n",
    "test_data = './sentiment_analysis-project/data/test.csv'\n",
    "valid_data = './sentiment_analysis-project/data/validation.csv'\n",
    "\n",
    "file = './sentiment_analysis-project/multiclass_dataset.csv'\n",
    "\n",
    "def save_load_df(file:str):\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    else:\n",
    "        df = pd.concat(map(pd.read_csv, [train_data, test_data, valid_data]), axis= 0, ignore_index=True)\n",
    "        df.to_csv(file, columns= ['id', 'text', 'label', 'sentiment'])\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "    return df\n",
    "\n",
    "df = save_load_df(file=file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP1, TP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess():\n",
    "    def __init__(self):\n",
    "        self.text_pattern = re.compile(\n",
    "        r'(<.+?>)'         # Balises HTML\n",
    "        r'(#|@)\\w+'  # @ and # words\n",
    "        r'|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'  # Emails\n",
    "        r'|(https?://[^\\s\\n\\r]+)' # URLs commençant par http ou https\n",
    "        r'|(www\\.[^\\s]+)'      # URLs commençant par www\n",
    "        r'|([\\U00010000-\\U0010ffff])'  # Émojis et autres caractères au-delà de l'ASCII étendu\n",
    "        r'|([^\\x00-\\xFF])'     # Tout ce qui n'est pas en ASCII étendu (0-255)\n",
    "        )\n",
    "        self.emoji_pattern = re.compile(\n",
    "            \"[\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols and Pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "            \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "            \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "            \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "            \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "            \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "            \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "            \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.punctuation = set(string.punctuation)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = self.text_pattern.sub('', str(text))\n",
    "        text = self.emoji_pattern.sub('', str(text))\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    \n",
    "    def get_tokens(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                if word not in self.stop_words:\n",
    "                    # clean all the punctuation and the StopWords\n",
    "                    word = ''.join([c for c in word if c not in self.punctuation])\n",
    "                    if word == '':\n",
    "                        continue\n",
    "                    tokens.append(word)\n",
    "        return tokens\n",
    "\n",
    "    def lemmetize_with_pos(self, tokens):\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        lemmes = [] \n",
    "        pos_tag = []\n",
    "        for token, pos in pos_tags:\n",
    "            if pos.startswith('J'):\n",
    "                lemma = self.lemmatizer.lemmatize(token, pos = 'a')\n",
    "            elif pos.startswith('V'):\n",
    "                lemma =  self.lemmatizer.lemmatize(token, pos = 'v')\n",
    "            elif pos.startswith('RB'):\n",
    "                lemma = self.lemmatizer.lemmatize(token, pos = 'r')\n",
    "            elif pos.startswith('N'):\n",
    "                lemma = self.lemmatizer.lemmatize(token, pos = 'n')\n",
    "            else:\n",
    "                lemma = self.lemmatizer.lemmatize(token)\n",
    "            lemmes.append(lemma)\n",
    "            pos_tag.append(pos)\n",
    "        return lemmes, pos_tag\n",
    "    \n",
    "    def get_lemmes(self, text):\n",
    "        tokens = self.get_tokens(text)\n",
    "        lemmes, _ = self.lemmetize_with_pos(tokens)\n",
    "        return lemmes\n",
    "    \n",
    "    def visualize_data(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        tokens = self.get_tokens(text)\n",
    "        lemmes, pos_tag = self.lemmetize_with_pos(tokens)\n",
    "        data = [[token, lemme, pos] for token, lemme, pos in zip(tokens, lemmes, pos_tag)]\n",
    "        return data\n",
    "    \n",
    "texts = list(df['text'])\n",
    "labels = list(df['label'])\n",
    "process_text = PreProcess()\n",
    "corpus = pd.DataFrame(data = [(process_text.get_tokens(text), label) for text, label in zip(texts, labels)], columns=['sentence', 'label'])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cook, microwave, pizza, yummy]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[plan, allow, sub, task, show, widget]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[love, humor, reword, like, say, group, therap...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[naw, idk, ur, talkin]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[suck, hear, hate, day, like]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41502</th>\n",
       "      <td>[fuck, internet, damn, time, warner]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41503</th>\n",
       "      <td>[look, forward, android, 15, push, g1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41504</th>\n",
       "      <td>[good, waste, time]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41505</th>\n",
       "      <td>[u, great, always, east, germany, noko, least,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41506</th>\n",
       "      <td>[love, dessert, use, live, live, tx, visit]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41507 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "0                        [cook, microwave, pizza, yummy]      2\n",
       "1                 [plan, allow, sub, task, show, widget]      1\n",
       "2      [love, humor, reword, like, say, group, therap...      2\n",
       "3                                 [naw, idk, ur, talkin]      1\n",
       "4                          [suck, hear, hate, day, like]      0\n",
       "...                                                  ...    ...\n",
       "41502               [fuck, internet, damn, time, warner]      0\n",
       "41503             [look, forward, android, 15, push, g1]      1\n",
       "41504                                [good, waste, time]      0\n",
       "41505  [u, great, always, east, germany, noko, least,...      2\n",
       "41506        [love, dessert, use, live, live, tx, visit]      2\n",
       "\n",
       "[41507 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the empty rows\n",
    "corpus = corpus[corpus['sentence'].str.len() != 0]\n",
    "corpus = corpus.reset_index(drop = True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_file = './sentiment_analysis-project/GoogleNews-vectors-negative300.bin'\n",
    "sentences = list(corpus['sentence'])\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_file, binary= True)\n",
    "\n",
    "def sentence_to_vec(sentence):\n",
    "    sentence_to_vec = [word2vec_model[word] for word in sentence if word in word2vec_model]\n",
    "    return sentence_to_vec\n",
    "\n",
    "df1 = pd.DataFrame(data = [(sentence_to_vec(sentence), label) for sentence, label in zip(sentences, list(corpus['label']))], \n",
    "                   columns=['vec', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.12792969, 0.047851562, 0.106933594, 0.0017...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.123535156, 0.031982422, 0.15039062, 0.1523...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.045654297, -0.14550781, 0.15625, 0.1660156...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.045654297, -0.14550781, 0.15625, 0.1660156...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.16699219, -0.05419922, -0.087402344, 0.019...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41352</th>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41353</th>\n",
       "      <td>[[0.38085938, 0.16113281, 0.22558594, 0.184570...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41354</th>\n",
       "      <td>[[-0.12207031, -0.16796875, 0.020629883, 0.160...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41355</th>\n",
       "      <td>[[-0.1796875, 0.057128906, 0.14160156, -0.0771...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41356</th>\n",
       "      <td>[[0.13476562, -0.02355957, 0.091796875, -0.070...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41357 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     vec  label\n",
       "0      [[0.12792969, 0.047851562, 0.106933594, 0.0017...      1\n",
       "1      [[0.123535156, 0.031982422, 0.15039062, 0.1523...      1\n",
       "2      [[0.045654297, -0.14550781, 0.15625, 0.1660156...      2\n",
       "3      [[0.045654297, -0.14550781, 0.15625, 0.1660156...      2\n",
       "4      [[0.16699219, -0.05419922, -0.087402344, 0.019...      2\n",
       "...                                                  ...    ...\n",
       "41352  [[0.103027344, -0.15234375, 0.025878906, 0.165...      2\n",
       "41353  [[0.38085938, 0.16113281, 0.22558594, 0.184570...      1\n",
       "41354  [[-0.12207031, -0.16796875, 0.020629883, 0.160...      1\n",
       "41355  [[-0.1796875, 0.057128906, 0.14160156, -0.0771...      2\n",
       "41356  [[0.13476562, -0.02355957, 0.091796875, -0.070...      2\n",
       "\n",
       "[41357 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1[df1['vec'].str.len() != 0]\n",
    "# df1 = df1.sort_values(by = 'vec', key = lambda vec: vec.str.len(), ascending=False)\n",
    "df1 = df1.reset_index(drop= True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.31835938, 0.059570312, -0.22949219, 0.087...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.048583984, 0.14355469, 0.22851562, 0.26953...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.048583984, 0.14355469, 0.22851562, 0.26953...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.10546875, -0.13574219, -0.12402344, 0.042...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.040527344, 0.0625, -0.017456055, 0.0786132...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41299</th>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41300</th>\n",
       "      <td>[[0.38085938, 0.16113281, 0.22558594, 0.184570...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41301</th>\n",
       "      <td>[[-0.12207031, -0.16796875, 0.020629883, 0.160...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41302</th>\n",
       "      <td>[[-0.1796875, 0.057128906, 0.14160156, -0.0771...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41303</th>\n",
       "      <td>[[0.13476562, -0.02355957, 0.091796875, -0.070...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     vec  label\n",
       "0      [[-0.31835938, 0.059570312, -0.22949219, 0.087...      1\n",
       "1      [[0.048583984, 0.14355469, 0.22851562, 0.26953...      1\n",
       "2      [[0.048583984, 0.14355469, 0.22851562, 0.26953...      1\n",
       "3      [[-0.10546875, -0.13574219, -0.12402344, 0.042...      0\n",
       "4      [[0.040527344, 0.0625, -0.017456055, 0.0786132...      0\n",
       "...                                                  ...    ...\n",
       "41299  [[0.103027344, -0.15234375, 0.025878906, 0.165...      2\n",
       "41300  [[0.38085938, 0.16113281, 0.22558594, 0.184570...      1\n",
       "41301  [[-0.12207031, -0.16796875, 0.020629883, 0.160...      1\n",
       "41302  [[-0.1796875, 0.057128906, 0.14160156, -0.0771...      2\n",
       "41303  [[0.13476562, -0.02355957, 0.091796875, -0.070...      2\n",
       "\n",
       "[41304 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [len(vec) for vec in list(df1['vec']) if len(vec) <= 64]\n",
    "# df1 = df1[df1['vec'].str.len() <= 64]\n",
    "# df1 = df1.reset_index(drop= True)\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41304, 64, 300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pad_sequence([torch.from_numpy(np.array(vec)) for vec in list(df1['vec'])], batch_first=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41304])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.from_numpy(np.array(list(df1['label'])))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=42, shuffle=True)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size=0.5, train_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126147 parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 || train_loss 1.065, valid_loss 1.021, train_accu 0.411 , valid_accu 0.463, dt= 1.76\n",
      "epoch 1 || train_loss 1.034, valid_loss 1.022, train_accu 0.480 , valid_accu 0.476, dt= 1.91\n",
      "epoch 2 || train_loss 1.016, valid_loss 1.011, train_accu 0.491 , valid_accu 0.475, dt= 1.97\n",
      "epoch 3 || train_loss 1.014, valid_loss 1.005, train_accu 0.490 , valid_accu 0.486, dt= 1.95\n",
      "epoch 4 || train_loss 1.008, valid_loss 1.005, train_accu 0.497 , valid_accu 0.483, dt= 1.92\n",
      "epoch 5 || train_loss 1.005, valid_loss 1.000, train_accu 0.498 , valid_accu 0.484, dt= 1.93\n",
      "epoch 6 || train_loss 1.000, valid_loss 0.998, train_accu 0.504 , valid_accu 0.503, dt= 1.91\n",
      "epoch 7 || train_loss 1.013, valid_loss 0.997, train_accu 0.487 , valid_accu 0.515, dt= 1.96\n",
      "epoch 8 || train_loss 0.983, valid_loss 0.969, train_accu 0.539 , valid_accu 0.548, dt= 1.92\n",
      "epoch 9 || train_loss 0.988, valid_loss 0.984, train_accu 0.542 , valid_accu 0.539, dt= 1.89\n",
      "epoch 10 || train_loss 1.028, valid_loss 1.043, train_accu 0.496 , valid_accu 0.465, dt= 1.94\n",
      "epoch 11 || train_loss 1.009, valid_loss 1.002, train_accu 0.503 , valid_accu 0.484, dt= 1.90\n",
      "epoch 12 || train_loss 1.000, valid_loss 1.002, train_accu 0.502 , valid_accu 0.482, dt= 1.96\n",
      "epoch 13 || train_loss 1.009, valid_loss 1.011, train_accu 0.494 , valid_accu 0.485, dt= 1.90\n",
      "epoch 14 || train_loss 1.006, valid_loss 0.998, train_accu 0.500 , valid_accu 0.491, dt= 1.88\n",
      "epoch 15 || train_loss 1.083, valid_loss 1.029, train_accu 0.450 , valid_accu 0.475, dt= 1.91\n",
      "epoch 16 || train_loss 1.016, valid_loss 1.035, train_accu 0.487 , valid_accu 0.490, dt= 1.91\n",
      "epoch 17 || train_loss 0.992, valid_loss 0.985, train_accu 0.533 , valid_accu 0.544, dt= 1.93\n",
      "epoch 18 || train_loss 0.995, valid_loss 1.002, train_accu 0.541 , valid_accu 0.506, dt= 1.94\n",
      "epoch 19 || train_loss 1.063, valid_loss 1.051, train_accu 0.435 , valid_accu 0.416, dt= 1.92\n",
      "epoch 20 || train_loss 1.056, valid_loss 1.037, train_accu 0.418 , valid_accu 0.437, dt= 1.90\n",
      "epoch 21 || train_loss 1.035, valid_loss 1.021, train_accu 0.460 , valid_accu 0.471, dt= 1.95\n",
      "epoch 22 || train_loss 1.028, valid_loss 1.023, train_accu 0.474 , valid_accu 0.456, dt= 1.88\n",
      "epoch 23 || train_loss 1.031, valid_loss 1.014, train_accu 0.469 , valid_accu 0.481, dt= 1.94\n",
      "epoch 24 || train_loss 1.013, valid_loss 0.997, train_accu 0.506 , valid_accu 0.518, dt= 1.92\n",
      "epoch 25 || train_loss 1.005, valid_loss 0.989, train_accu 0.533 , valid_accu 0.542, dt= 1.92\n",
      "epoch 26 || train_loss 0.996, valid_loss 1.001, train_accu 0.542 , valid_accu 0.515, dt= 1.89\n",
      "epoch 27 || train_loss 1.012, valid_loss 1.014, train_accu 0.510 , valid_accu 0.488, dt= 1.87\n",
      "epoch 28 || train_loss 1.046, valid_loss 1.101, train_accu 0.472 , valid_accu 0.393, dt= 1.92\n",
      "epoch 29 || train_loss 1.101, valid_loss 1.097, train_accu 0.417 , valid_accu 0.404, dt= 1.93\n",
      "epoch 30 || train_loss 1.102, valid_loss 1.087, train_accu 0.414 , valid_accu 0.417, dt= 1.98\n",
      "epoch 31 || train_loss 1.088, valid_loss 1.079, train_accu 0.439 , valid_accu 0.430, dt= 1.92\n",
      "epoch 32 || train_loss 1.043, valid_loss 0.968, train_accu 0.485 , valid_accu 0.558, dt= 1.90\n",
      "epoch 33 || train_loss 0.956, valid_loss 0.945, train_accu 0.593 , valid_accu 0.590, dt= 1.97\n",
      "epoch 34 || train_loss 0.950, valid_loss 0.959, train_accu 0.587 , valid_accu 0.555, dt= 1.90\n",
      "epoch 35 || train_loss 0.945, valid_loss 0.926, train_accu 0.591 , valid_accu 0.607, dt= 1.91\n",
      "epoch 36 || train_loss 0.950, valid_loss 0.943, train_accu 0.569 , valid_accu 0.570, dt= 1.98\n",
      "epoch 37 || train_loss 0.926, valid_loss 0.916, train_accu 0.613 , valid_accu 0.618, dt= 1.94\n",
      "epoch 38 || train_loss 0.907, valid_loss 0.910, train_accu 0.640 , valid_accu 0.623, dt= 1.94\n",
      "epoch 39 || train_loss 0.959, valid_loss 0.912, train_accu 0.564 , valid_accu 0.626, dt= 1.93\n",
      "epoch 40 || train_loss 0.910, valid_loss 0.909, train_accu 0.638 , valid_accu 0.627, dt= 1.92\n",
      "epoch 41 || train_loss 0.912, valid_loss 0.920, train_accu 0.635 , valid_accu 0.607, dt= 1.92\n",
      "epoch 42 || train_loss 0.910, valid_loss 0.919, train_accu 0.636 , valid_accu 0.613, dt= 1.92\n",
      "epoch 43 || train_loss 0.917, valid_loss 0.923, train_accu 0.622 , valid_accu 0.600, dt= 1.91\n",
      "epoch 44 || train_loss 0.915, valid_loss 0.914, train_accu 0.626 , valid_accu 0.614, dt= 1.91\n",
      "epoch 45 || train_loss 0.911, valid_loss 0.917, train_accu 0.635 , valid_accu 0.617, dt= 1.94\n",
      "epoch 46 || train_loss 0.990, valid_loss 0.975, train_accu 0.515 , valid_accu 0.502, dt= 1.90\n",
      "epoch 47 || train_loss 0.961, valid_loss 0.971, train_accu 0.538 , valid_accu 0.508, dt= 1.96\n",
      "epoch 48 || train_loss 0.971, valid_loss 0.951, train_accu 0.537 , valid_accu 0.568, dt= 1.89\n",
      "epoch 49 || train_loss 0.923, valid_loss 0.920, train_accu 0.614 , valid_accu 0.606, dt= 1.95\n",
      "epoch 50 || train_loss 0.913, valid_loss 0.913, train_accu 0.627 , valid_accu 0.608, dt= 1.93\n",
      "epoch 51 || train_loss 0.904, valid_loss 0.906, train_accu 0.635 , valid_accu 0.616, dt= 1.95\n",
      "epoch 52 || train_loss 0.903, valid_loss 0.964, train_accu 0.631 , valid_accu 0.586, dt= 1.93\n",
      "epoch 53 || train_loss 0.934, valid_loss 0.926, train_accu 0.603 , valid_accu 0.595, dt= 1.92\n",
      "epoch 54 || train_loss 0.920, valid_loss 0.921, train_accu 0.615 , valid_accu 0.604, dt= 1.88\n",
      "epoch 55 || train_loss 0.919, valid_loss 0.917, train_accu 0.616 , valid_accu 0.609, dt= 1.92\n",
      "epoch 56 || train_loss 0.905, valid_loss 0.904, train_accu 0.635 , valid_accu 0.626, dt= 1.90\n",
      "epoch 57 || train_loss 0.894, valid_loss 0.901, train_accu 0.650 , valid_accu 0.626, dt= 1.91\n",
      "epoch 58 || train_loss 0.933, valid_loss 0.948, train_accu 0.604 , valid_accu 0.579, dt= 1.96\n",
      "epoch 59 || train_loss 0.945, valid_loss 0.933, train_accu 0.595 , valid_accu 0.596, dt= 1.90\n",
      "epoch 60 || train_loss 0.927, valid_loss 0.920, train_accu 0.613 , valid_accu 0.609, dt= 1.93\n",
      "epoch 61 || train_loss 0.914, valid_loss 0.915, train_accu 0.628 , valid_accu 0.611, dt= 1.92\n",
      "epoch 62 || train_loss 0.907, valid_loss 0.911, train_accu 0.634 , valid_accu 0.615, dt= 1.87\n",
      "epoch 63 || train_loss 0.912, valid_loss 0.919, train_accu 0.630 , valid_accu 0.612, dt= 1.90\n",
      "epoch 64 || train_loss 0.904, valid_loss 0.916, train_accu 0.639 , valid_accu 0.612, dt= 1.90\n",
      "epoch 65 || train_loss 0.901, valid_loss 0.910, train_accu 0.644 , valid_accu 0.618, dt= 1.95\n",
      "epoch 66 || train_loss 0.896, valid_loss 0.907, train_accu 0.647 , valid_accu 0.620, dt= 1.95\n",
      "epoch 67 || train_loss 0.906, valid_loss 0.920, train_accu 0.633 , valid_accu 0.608, dt= 1.94\n",
      "epoch 68 || train_loss 0.907, valid_loss 0.912, train_accu 0.634 , valid_accu 0.615, dt= 1.93\n",
      "epoch 69 || train_loss 0.904, valid_loss 0.918, train_accu 0.636 , valid_accu 0.615, dt= 1.92\n",
      "epoch 70 || train_loss 0.901, valid_loss 0.910, train_accu 0.640 , valid_accu 0.615, dt= 1.88\n",
      "epoch 71 || train_loss 0.897, valid_loss 0.907, train_accu 0.643 , valid_accu 0.619, dt= 1.83\n",
      "epoch 72 || train_loss 0.893, valid_loss 0.899, train_accu 0.648 , valid_accu 0.627, dt= 1.89\n",
      "epoch 73 || train_loss 0.887, valid_loss 0.905, train_accu 0.655 , valid_accu 0.626, dt= 1.87\n",
      "epoch 74 || train_loss 0.902, valid_loss 0.911, train_accu 0.641 , valid_accu 0.613, dt= 1.92\n",
      "epoch 75 || train_loss 0.896, valid_loss 0.909, train_accu 0.645 , valid_accu 0.619, dt= 1.95\n",
      "epoch 76 || train_loss 0.887, valid_loss 0.897, train_accu 0.654 , valid_accu 0.629, dt= 1.89\n",
      "epoch 77 || train_loss 0.885, valid_loss 0.897, train_accu 0.658 , valid_accu 0.628, dt= 1.94\n",
      "epoch 78 || train_loss 0.883, valid_loss 0.886, train_accu 0.659 , valid_accu 0.642, dt= 1.92\n",
      "epoch 79 || train_loss 0.888, valid_loss 0.900, train_accu 0.653 , valid_accu 0.627, dt= 1.89\n",
      "epoch 80 || train_loss 0.888, valid_loss 0.902, train_accu 0.653 , valid_accu 0.624, dt= 1.91\n",
      "epoch 81 || train_loss 0.893, valid_loss 0.906, train_accu 0.648 , valid_accu 0.620, dt= 1.93\n",
      "epoch 82 || train_loss 0.887, valid_loss 0.891, train_accu 0.655 , valid_accu 0.636, dt= 1.96\n",
      "epoch 83 || train_loss 0.871, valid_loss 0.875, train_accu 0.672 , valid_accu 0.650, dt= 1.90\n",
      "epoch 84 || train_loss 0.866, valid_loss 0.881, train_accu 0.674 , valid_accu 0.644, dt= 1.91\n",
      "epoch 85 || train_loss 0.870, valid_loss 0.877, train_accu 0.670 , valid_accu 0.652, dt= 1.90\n",
      "epoch 86 || train_loss 0.860, valid_loss 0.870, train_accu 0.680 , valid_accu 0.661, dt= 1.91\n",
      "epoch 87 || train_loss 0.859, valid_loss 0.870, train_accu 0.682 , valid_accu 0.656, dt= 1.94\n",
      "epoch 88 || train_loss 0.857, valid_loss 0.867, train_accu 0.683 , valid_accu 0.658, dt= 1.93\n",
      "epoch 89 || train_loss 0.862, valid_loss 0.873, train_accu 0.680 , valid_accu 0.653, dt= 1.90\n",
      "epoch 90 || train_loss 0.861, valid_loss 0.874, train_accu 0.681 , valid_accu 0.652, dt= 1.89\n",
      "epoch 91 || train_loss 0.855, valid_loss 0.867, train_accu 0.687 , valid_accu 0.658, dt= 1.95\n",
      "epoch 92 || train_loss 0.853, valid_loss 0.866, train_accu 0.688 , valid_accu 0.661, dt= 1.91\n",
      "epoch 93 || train_loss 0.851, valid_loss 0.923, train_accu 0.692 , valid_accu 0.600, dt= 1.93\n",
      "epoch 94 || train_loss 0.899, valid_loss 0.886, train_accu 0.639 , valid_accu 0.644, dt= 1.95\n",
      "epoch 95 || train_loss 0.864, valid_loss 0.878, train_accu 0.678 , valid_accu 0.651, dt= 1.94\n",
      "epoch 96 || train_loss 0.855, valid_loss 0.871, train_accu 0.686 , valid_accu 0.654, dt= 1.93\n",
      "epoch 97 || train_loss 0.851, valid_loss 0.869, train_accu 0.692 , valid_accu 0.654, dt= 1.91\n",
      "epoch 98 || train_loss 0.848, valid_loss 0.867, train_accu 0.693 , valid_accu 0.658, dt= 1.89\n",
      "epoch 99 || train_loss 0.846, valid_loss 0.867, train_accu 0.695 , valid_accu 0.658, dt= 1.89\n",
      "epoch 100 || train_loss 0.844, valid_loss 0.864, train_accu 0.696 , valid_accu 0.663, dt= 1.94\n",
      "epoch 101 || train_loss 0.843, valid_loss 0.863, train_accu 0.699 , valid_accu 0.663, dt= 1.90\n",
      "epoch 102 || train_loss 0.842, valid_loss 0.862, train_accu 0.700 , valid_accu 0.665, dt= 1.89\n",
      "epoch 103 || train_loss 0.840, valid_loss 0.859, train_accu 0.702 , valid_accu 0.663, dt= 1.90\n",
      "epoch 104 || train_loss 0.841, valid_loss 0.861, train_accu 0.701 , valid_accu 0.666, dt= 1.91\n",
      "epoch 105 || train_loss 0.841, valid_loss 0.861, train_accu 0.703 , valid_accu 0.662, dt= 1.90\n",
      "epoch 106 || train_loss 0.838, valid_loss 0.859, train_accu 0.706 , valid_accu 0.669, dt= 1.88\n",
      "epoch 107 || train_loss 0.837, valid_loss 0.859, train_accu 0.707 , valid_accu 0.670, dt= 1.86\n",
      "epoch 108 || train_loss 0.837, valid_loss 0.862, train_accu 0.706 , valid_accu 0.664, dt= 1.91\n",
      "epoch 109 || train_loss 0.836, valid_loss 0.858, train_accu 0.706 , valid_accu 0.666, dt= 1.85\n",
      "epoch 110 || train_loss 0.833, valid_loss 0.857, train_accu 0.710 , valid_accu 0.670, dt= 1.90\n",
      "epoch 111 || train_loss 0.831, valid_loss 0.856, train_accu 0.712 , valid_accu 0.671, dt= 1.88\n",
      "epoch 112 || train_loss 0.830, valid_loss 0.855, train_accu 0.713 , valid_accu 0.669, dt= 1.90\n",
      "epoch 113 || train_loss 0.832, valid_loss 0.862, train_accu 0.710 , valid_accu 0.662, dt= 1.87\n",
      "epoch 114 || train_loss 0.827, valid_loss 0.856, train_accu 0.716 , valid_accu 0.671, dt= 1.87\n",
      "epoch 115 || train_loss 0.825, valid_loss 0.851, train_accu 0.718 , valid_accu 0.676, dt= 1.90\n",
      "epoch 116 || train_loss 0.830, valid_loss 0.860, train_accu 0.712 , valid_accu 0.668, dt= 1.88\n",
      "epoch 117 || train_loss 0.826, valid_loss 0.850, train_accu 0.716 , valid_accu 0.683, dt= 1.89\n",
      "epoch 118 || train_loss 0.822, valid_loss 0.848, train_accu 0.722 , valid_accu 0.680, dt= 1.89\n",
      "epoch 119 || train_loss 0.820, valid_loss 0.851, train_accu 0.722 , valid_accu 0.677, dt= 1.87\n",
      "epoch 120 || train_loss 0.821, valid_loss 0.847, train_accu 0.724 , valid_accu 0.679, dt= 1.92\n",
      "epoch 121 || train_loss 0.819, valid_loss 0.848, train_accu 0.725 , valid_accu 0.683, dt= 1.89\n",
      "epoch 122 || train_loss 0.815, valid_loss 0.848, train_accu 0.730 , valid_accu 0.682, dt= 1.87\n",
      "epoch 123 || train_loss 0.817, valid_loss 0.847, train_accu 0.728 , valid_accu 0.684, dt= 1.88\n",
      "epoch 124 || train_loss 0.811, valid_loss 0.843, train_accu 0.734 , valid_accu 0.688, dt= 1.88\n",
      "epoch 125 || train_loss 0.813, valid_loss 0.843, train_accu 0.733 , valid_accu 0.688, dt= 1.93\n",
      "epoch 126 || train_loss 0.811, valid_loss 0.847, train_accu 0.734 , valid_accu 0.684, dt= 1.89\n",
      "epoch 127 || train_loss 0.809, valid_loss 0.843, train_accu 0.737 , valid_accu 0.690, dt= 1.86\n",
      "epoch 128 || train_loss 0.806, valid_loss 0.842, train_accu 0.741 , valid_accu 0.691, dt= 1.90\n",
      "epoch 129 || train_loss 0.803, valid_loss 0.847, train_accu 0.744 , valid_accu 0.685, dt= 1.88\n",
      "epoch 130 || train_loss 0.801, valid_loss 0.842, train_accu 0.744 , valid_accu 0.692, dt= 1.65\n",
      "epoch 131 || train_loss 0.800, valid_loss 0.842, train_accu 0.746 , valid_accu 0.692, dt= 1.66\n",
      "epoch 132 || train_loss 0.800, valid_loss 0.843, train_accu 0.746 , valid_accu 0.694, dt= 1.67\n",
      "epoch 133 || train_loss 0.799, valid_loss 0.844, train_accu 0.748 , valid_accu 0.691, dt= 1.68\n",
      "epoch 134 || train_loss 0.793, valid_loss 0.843, train_accu 0.755 , valid_accu 0.691, dt= 1.62\n",
      "epoch 135 || train_loss 0.794, valid_loss 0.841, train_accu 0.755 , valid_accu 0.693, dt= 1.61\n",
      "epoch 136 || train_loss 0.790, valid_loss 0.840, train_accu 0.758 , valid_accu 0.693, dt= 1.58\n",
      "epoch 137 || train_loss 0.790, valid_loss 0.839, train_accu 0.758 , valid_accu 0.696, dt= 1.63\n",
      "epoch 138 || train_loss 0.788, valid_loss 0.841, train_accu 0.760 , valid_accu 0.693, dt= 1.62\n",
      "epoch 139 || train_loss 0.786, valid_loss 0.840, train_accu 0.763 , valid_accu 0.694, dt= 1.58\n",
      "epoch 140 || train_loss 0.786, valid_loss 0.839, train_accu 0.762 , valid_accu 0.696, dt= 1.58\n",
      "epoch 141 || train_loss 0.785, valid_loss 0.841, train_accu 0.763 , valid_accu 0.693, dt= 1.55\n",
      "epoch 142 || train_loss 0.785, valid_loss 0.841, train_accu 0.764 , valid_accu 0.694, dt= 1.56\n",
      "epoch 143 || train_loss 0.785, valid_loss 0.837, train_accu 0.763 , valid_accu 0.698, dt= 1.60\n",
      "epoch 144 || train_loss 0.783, valid_loss 0.838, train_accu 0.765 , valid_accu 0.695, dt= 1.62\n",
      "epoch 145 || train_loss 0.784, valid_loss 0.834, train_accu 0.765 , valid_accu 0.702, dt= 1.64\n",
      "epoch 146 || train_loss 0.780, valid_loss 0.837, train_accu 0.769 , valid_accu 0.699, dt= 1.59\n",
      "epoch 147 || train_loss 0.781, valid_loss 0.837, train_accu 0.768 , valid_accu 0.699, dt= 1.60\n",
      "epoch 148 || train_loss 0.777, valid_loss 0.840, train_accu 0.772 , valid_accu 0.695, dt= 1.62\n",
      "epoch 149 || train_loss 0.777, valid_loss 0.846, train_accu 0.772 , valid_accu 0.689, dt= 1.61\n",
      "epoch 150 || train_loss 0.778, valid_loss 0.835, train_accu 0.770 , valid_accu 0.700, dt= 1.60\n",
      "epoch 151 || train_loss 0.781, valid_loss 0.835, train_accu 0.768 , valid_accu 0.702, dt= 1.60\n",
      "epoch 152 || train_loss 0.775, valid_loss 0.836, train_accu 0.774 , valid_accu 0.700, dt= 1.60\n",
      "epoch 153 || train_loss 0.773, valid_loss 0.833, train_accu 0.776 , valid_accu 0.701, dt= 1.60\n",
      "epoch 154 || train_loss 0.773, valid_loss 0.834, train_accu 0.776 , valid_accu 0.702, dt= 1.62\n",
      "epoch 155 || train_loss 0.774, valid_loss 0.833, train_accu 0.775 , valid_accu 0.703, dt= 1.62\n",
      "epoch 156 || train_loss 0.771, valid_loss 0.836, train_accu 0.779 , valid_accu 0.700, dt= 1.61\n",
      "epoch 157 || train_loss 0.772, valid_loss 0.846, train_accu 0.777 , valid_accu 0.690, dt= 1.61\n",
      "epoch 158 || train_loss 0.771, valid_loss 0.834, train_accu 0.779 , valid_accu 0.703, dt= 1.62\n",
      "epoch 159 || train_loss 0.769, valid_loss 0.842, train_accu 0.781 , valid_accu 0.694, dt= 1.60\n",
      "epoch 160 || train_loss 0.768, valid_loss 0.836, train_accu 0.782 , valid_accu 0.701, dt= 1.62\n",
      "epoch 161 || train_loss 0.768, valid_loss 0.836, train_accu 0.781 , valid_accu 0.702, dt= 1.60\n",
      "epoch 162 || train_loss 0.768, valid_loss 0.836, train_accu 0.781 , valid_accu 0.700, dt= 1.63\n",
      "epoch 163 || train_loss 0.768, valid_loss 0.835, train_accu 0.781 , valid_accu 0.701, dt= 1.64\n",
      "epoch 164 || train_loss 0.768, valid_loss 0.836, train_accu 0.781 , valid_accu 0.699, dt= 1.62\n",
      "epoch 165 || train_loss 0.766, valid_loss 0.834, train_accu 0.784 , valid_accu 0.704, dt= 1.61\n",
      "epoch 166 || train_loss 0.769, valid_loss 0.835, train_accu 0.781 , valid_accu 0.702, dt= 1.58\n",
      "epoch 167 || train_loss 0.767, valid_loss 0.834, train_accu 0.783 , valid_accu 0.701, dt= 1.58\n",
      "epoch 168 || train_loss 0.762, valid_loss 0.835, train_accu 0.788 , valid_accu 0.700, dt= 1.60\n",
      "epoch 169 || train_loss 0.764, valid_loss 0.833, train_accu 0.786 , valid_accu 0.704, dt= 1.60\n",
      "epoch 170 || train_loss 0.762, valid_loss 0.832, train_accu 0.787 , valid_accu 0.706, dt= 1.58\n",
      "epoch 171 || train_loss 0.765, valid_loss 0.835, train_accu 0.784 , valid_accu 0.699, dt= 1.64\n",
      "epoch 172 || train_loss 0.761, valid_loss 0.833, train_accu 0.788 , valid_accu 0.703, dt= 1.78\n",
      "epoch 173 || train_loss 0.761, valid_loss 0.831, train_accu 0.789 , valid_accu 0.705, dt= 1.76\n",
      "epoch 174 || train_loss 0.776, valid_loss 0.832, train_accu 0.773 , valid_accu 0.703, dt= 1.77\n",
      "epoch 175 || train_loss 0.760, valid_loss 0.835, train_accu 0.790 , valid_accu 0.701, dt= 1.60\n",
      "epoch 176 || train_loss 0.761, valid_loss 0.838, train_accu 0.788 , valid_accu 0.699, dt= 1.61\n",
      "epoch 177 || train_loss 0.760, valid_loss 0.838, train_accu 0.790 , valid_accu 0.700, dt= 1.59\n",
      "epoch 178 || train_loss 0.756, valid_loss 0.837, train_accu 0.794 , valid_accu 0.700, dt= 1.60\n",
      "epoch 179 || train_loss 0.755, valid_loss 0.833, train_accu 0.795 , valid_accu 0.704, dt= 1.59\n",
      "epoch 180 || train_loss 0.757, valid_loss 0.831, train_accu 0.792 , valid_accu 0.706, dt= 1.60\n",
      "epoch 181 || train_loss 0.757, valid_loss 0.832, train_accu 0.793 , valid_accu 0.705, dt= 1.62\n",
      "epoch 182 || train_loss 0.756, valid_loss 0.843, train_accu 0.794 , valid_accu 0.695, dt= 1.62\n",
      "epoch 183 || train_loss 0.758, valid_loss 0.835, train_accu 0.791 , valid_accu 0.700, dt= 1.62\n",
      "epoch 184 || train_loss 0.753, valid_loss 0.833, train_accu 0.797 , valid_accu 0.703, dt= 1.61\n",
      "epoch 185 || train_loss 0.759, valid_loss 0.834, train_accu 0.791 , valid_accu 0.702, dt= 1.60\n",
      "epoch 186 || train_loss 0.758, valid_loss 0.834, train_accu 0.792 , valid_accu 0.703, dt= 1.62\n",
      "epoch 187 || train_loss 0.756, valid_loss 0.829, train_accu 0.794 , valid_accu 0.708, dt= 1.64\n",
      "epoch 188 || train_loss 0.756, valid_loss 0.832, train_accu 0.794 , valid_accu 0.706, dt= 1.61\n",
      "epoch 189 || train_loss 0.753, valid_loss 0.830, train_accu 0.798 , valid_accu 0.707, dt= 1.59\n",
      "epoch 190 || train_loss 0.756, valid_loss 0.834, train_accu 0.794 , valid_accu 0.702, dt= 1.61\n",
      "epoch 191 || train_loss 0.753, valid_loss 0.829, train_accu 0.797 , valid_accu 0.708, dt= 1.62\n",
      "epoch 192 || train_loss 0.753, valid_loss 0.833, train_accu 0.796 , valid_accu 0.703, dt= 1.69\n",
      "epoch 193 || train_loss 0.751, valid_loss 0.833, train_accu 0.799 , valid_accu 0.704, dt= 1.69\n",
      "epoch 194 || train_loss 0.753, valid_loss 0.831, train_accu 0.798 , valid_accu 0.705, dt= 1.68\n",
      "epoch 195 || train_loss 0.752, valid_loss 0.830, train_accu 0.798 , valid_accu 0.707, dt= 1.65\n",
      "epoch 196 || train_loss 0.752, valid_loss 0.837, train_accu 0.798 , valid_accu 0.700, dt= 1.85\n",
      "epoch 197 || train_loss 0.756, valid_loss 0.832, train_accu 0.794 , valid_accu 0.705, dt= 1.93\n",
      "epoch 198 || train_loss 0.753, valid_loss 0.827, train_accu 0.796 , valid_accu 0.708, dt= 1.89\n",
      "epoch 199 || train_loss 0.756, valid_loss 0.834, train_accu 0.794 , valid_accu 0.703, dt= 1.97\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_dataset, batch_size= batch_size, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size = batch_size, shuffle=False, drop_last=True)\n",
    "test_dl  = DataLoader(test_dataset, batch_size= batch_size, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# (258, 195, 300)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim = 300,\n",
    "                 hidden_size = 128,\n",
    "                 bidirectional = False,\n",
    "                 num_layers = 1,\n",
    "                 dropout = 0.2\n",
    "        ):\n",
    "        super(Model, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size= embed_dim,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias= False,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.D = 1 if self.bidirectional == False else 2\n",
    "        self.fc2 = nn.Linear(self.D * hidden_size , 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, T, C = x.shape\n",
    "        out, (h1, c1) = self.lstm(x) \n",
    "        out = F.softmax(self.fc2(out), dim = -1)\n",
    "        return out\n",
    "\n",
    "torch.manual_seed(42) # for the distribution of the gradient\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = Model()\n",
    "model.to(device=device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 3e-4)\n",
    "print(sum(p.numel() for p in model.parameters()),'parameters')\n",
    "\n",
    "import time\n",
    "num_iter = 50\n",
    "for i in range(num_iter):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    t1 = time.time()\n",
    "    model.train()\n",
    "    for xb_train, yb_train in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        xb_train = xb_train.to(device)\n",
    "        yb_train = yb_train.to(device)\n",
    "        with torch.autocast(device_type = 'cuda', dtype = torch.float16):\n",
    "            logits= model(xb_train)[:, -1, :]\n",
    "            loss = F.cross_entropy(logits, yb_train)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * batch_size\n",
    "        train_acc += (torch.argmax(logits, dim = 1) == yb_train).float().sum().item()\n",
    "        # update the gradient\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dl.dataset)\n",
    "    train_acc /= len(train_dl.dataset)\n",
    "     \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for xb_valid, yb_valid in valid_dl:\n",
    "            xb_valid = xb_valid.to(device)\n",
    "            yb_valid = yb_valid.to(device)\n",
    "            logits = model(xb_valid)[:, -1, :]\n",
    "            loss = F.cross_entropy(logits, yb_valid)\n",
    "            valid_loss += loss.item() * batch_size\n",
    "            valid_acc += (torch.argmax(logits, dim = 1) == yb_valid).float().sum().item()\n",
    "    \n",
    "    valid_loss /= len(valid_dl.dataset)\n",
    "    valid_acc /= len(valid_dl.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "    t2 = time.time()\n",
    "    dt = (t2 - t1)\n",
    "    print(f'epoch {i} || train_loss {train_loss:.3f}, valid_loss {valid_loss:.3f}, train_accu {train_acc:.3f} , valid_accu {valid_acc:.3f}, dt= {dt:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(num_iter) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      4\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_arr = np.arange(num_iter) + 1\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, train_losses, '-o', label='Train loss')\n",
    "ax.plot(x_arr, valid_losses, '--<', label='Validation loss')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlabel('Epoch', size=10)\n",
    "ax.set_ylabel('Loss', size=10)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, train_accuracies, '-o', label='Train acc.')\n",
    "ax.plot(x_arr, valid_accuracies, '--<', label='Validation acc.')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlabel('Epoch', size=10)\n",
    "ax.set_ylabel('Accuracy', size=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.7116920842411039\n",
      "Precision: 0.7132969198502818\n",
      "Recall: 0.7116920842411039\n",
      "F1-Score: 0.7119781460902211\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.69      1180\n",
      "           1       0.66      0.70      0.68      1547\n",
      "           2       0.77      0.76      0.77      1404\n",
      "\n",
      "    accuracy                           0.71      4131\n",
      "   macro avg       0.72      0.71      0.71      4131\n",
      "weighted avg       0.71      0.71      0.71      4131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "test_acc = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for xb_test, yb_test in test_dl:\n",
    "        xb_test = xb_test.to(device)\n",
    "        yb_test = yb_test.to(device)\n",
    "        logits = model(xb_test)[:, -1, :]\n",
    "        pred = torch.argmax(logits, dim = 1)\n",
    "        test_acc += (pred == yb_test).float().sum().item()\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(yb_test.cpu().numpy())\n",
    "\n",
    "test_acc /= len(test_dl.dataset)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')  # 'weighted' pour pondérer selon les classes\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print('test accuracy', test_acc)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# train_dataset = TensorDataset(x_train, y_train)\n",
    "# test_dataset = TensorDataset(x_test, y_test)\n",
    "# valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "\n",
    "# batch_size = 128\n",
    "# train_dl = DataLoader(train_dataset, batch_size= batch_size, shuffle=True, drop_last=True)\n",
    "# valid_dl = DataLoader(valid_dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "# test_dl  = DataLoader(test_dataset, batch_size= batch_size, shuffle=True, drop_last= True)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# # (258, 195, 300)\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  embed_dim = 300,\n",
    "#                  hidden_size = 128,\n",
    "#                  bidirectional = False,\n",
    "#                  num_layers = 1,\n",
    "#                  dropout = 0.2\n",
    "#         ):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.bidirectional = bidirectional\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size= embed_dim,\n",
    "#                             hidden_size = hidden_size,\n",
    "#                             num_layers=num_layers,\n",
    "#                             bias= False,\n",
    "#                             bidirectional=bidirectional,\n",
    "#                             batch_first=True,\n",
    "#                             # dropout=dropout\n",
    "#         )\n",
    "#         self.D = 1 if self.bidirectional == False else 2\n",
    "#         self.fc2 = nn.Linear(self.D * hidden_size , 3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, T, C = x.shape\n",
    "#         out, (h1, c1) = self.lstm(x)\n",
    "#         out = F.softmax(self.fc2(out), dim = -1)\n",
    "#         return out\n",
    "\n",
    "# torch.manual_seed(42) # for the distribution of the gradient\n",
    "\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# model = Model()\n",
    "# model.to(device=device)\n",
    "# # model = torch.compile(model)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 3e-4)\n",
    "# print(sum(p.numel() for p in model.parameters()),'parameters')\n",
    "\n",
    "# import time\n",
    "# num_iter = 200\n",
    "# for i in range(num_iter):\n",
    "#     train_loss = 0\n",
    "#     train_acc = 0\n",
    "#     t1 = time.time()\n",
    "#     model.train()\n",
    "#     for xb_train, yb_train in train_dl:\n",
    "#         optimizer.zero_grad()\n",
    "#         # forward pass\n",
    "#         xb_train = xb_train.to(device)\n",
    "#         yb_train = yb_train.to(device)\n",
    "#         with torch.autocast(device_type = 'cuda', dtype = torch.float16):\n",
    "#             logits= model(xb_train)[:, -1, :]\n",
    "#             loss = F.cross_entropy(logits, yb_train)\n",
    "#         # backward pass\n",
    "#         loss.backward()\n",
    "#         train_loss += loss.item() * batch_size\n",
    "#         train_acc += (torch.argmax(logits, dim = 1) == yb_train).float().sum().item()\n",
    "#         # update the gradient\n",
    "#         optimizer.step()\n",
    "\n",
    "#     train_loss /= len(train_dl.dataset)\n",
    "#     train_acc /= len(train_dl.dataset)\n",
    "     \n",
    "#     model.eval()\n",
    "#     valid_loss = 0\n",
    "#     valid_acc = 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb_valid, yb_valid in valid_dl:\n",
    "#             xb_valid = xb_valid.to(device)\n",
    "#             yb_valid = yb_valid.to(device)\n",
    "#             logits = model(xb_valid)[:, -1, :]\n",
    "#             loss = F.cross_entropy(logits, yb_valid)\n",
    "#             valid_loss += loss.item() * batch_size\n",
    "#             valid_acc += (torch.argmax(logits, dim = 1) == yb_valid).float().sum().item()\n",
    "    \n",
    "#     valid_loss /= len(valid_dl.dataset)\n",
    "#     valid_acc /= len(valid_dl.dataset)\n",
    "#     t2 = time.time()\n",
    "#     dt = (t2 - t1)\n",
    "#     print(f'epoch {i} || train_loss {train_loss:.3f}, valid_loss {valid_loss:.3f}, train_accu {train_acc:.3f} , valid_accu {valid_acc:.3f}, dt= {dt:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
